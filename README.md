# ðŸ§  OpsIQ â€” Self-Improving Billing Anomaly Detection Agent

A multi-agent system that autonomously monitors business signals, detects billing anomalies (revenue leakage, duplicate refunds, tier mismatches), takes remediation actions, and **learns from human feedback** to improve over time â€” powered by LLM reasoning at every decision point.

## The Problem

Finance teams lose millions annually to billing anomalies â€” duplicate refunds, underbilling gaps, tier mismatches, manual credit abuse. These issues are caught late (if at all), investigated manually, and the same mistakes repeat because systems don't learn.

**OpsIQ solves this with a closed-loop autonomous agent:**

1. **Detect** â€” Ingest signals, run 5 anomaly detectors, score and rank findings
2. **Act** â€” Create remediation actions (cases, alerts, approval tasks) with audit trails
3. **Learn** â€” Human feedback flows through an LLM-powered memory agent that adjusts detection thresholds, penalties, and confidence scoring
4. **Improve** â€” Rerun triage with updated memory â†’ fewer false positives, better calibration

---

## Features

### Autonomous Investigation Pipeline
- Signal ingestion from monitoring sources â†’ LLM-powered investigation strategy
- 5 anomaly detectors: duplicate refunds, underbilling, tier mismatch, refund spikes, manual credits
- Severity/confidence/impact scoring with sentiment analysis on evidence text
- Remediation actions with workflow audit trails

### Self-Improvement Loop
- **Feedback capture** â€” approve, reject, false positive on each case
- **LLM-powered memory** â€” AI reasons about feedback to decide threshold adjustments
- **LLM-powered evaluation** â€” AI assesses run quality and generates calibration advice
- **Visible improvement** â€” rerun triage and see confidence downgrades, threshold adjustments, and impact penalties

### Natural Language Analyst
- Ask business questions in plain English
- Get answers with charts, SQL, confidence scores, and follow-up suggestions
- Revenue analysis, refund trends, underbilling by tier, regional breakdowns

### LLM Reasoning at Every Step
- **Orchestrator** â€” analyzes signals, decides investigation strategy, synthesizes findings
- **Memory Agent** â€” reasons about feedback to generate learning updates
- **Evaluator** â€” assesses run quality and generates calibration advice
- All reasoning traces visible in the UI for full observability

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Streamlit Frontend                      â”‚
â”‚  Mission Control â”‚ Triage Cases â”‚ Analyst â”‚ QA Lab       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FastAPI Backend                         â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Monitor  â”‚â†’â”‚  Triage    â”‚â†’â”‚  Action     â”‚           â”‚
â”‚  â”‚  Agent   â”‚  â”‚  Agent     â”‚  â”‚  Engine    â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚       â”‚           â”‚  â”‚              â”‚                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”˜  â”Œâ”€â”€â”€â”€â”€â”€â”  â”‚                    â”‚
â”‚  â”‚ Signal   â”‚ â”‚Anomalyâ”‚  â”‚Senti-â”‚  â”‚                    â”‚
â”‚  â”‚ Adapter  â”‚ â”‚+Score â”‚  â”‚ment  â”‚  â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚Tools  â”‚  â”‚Engineâ”‚  â”‚                    â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â”‚                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Metric   â”‚  â”‚Evaluator â”‚  â”‚ Memory   â”‚              â”‚
â”‚  â”‚ Layer    â”‚  â”‚  Agent   â”‚  â”‚  Agent   â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ (+ LLM)  â”‚  â”‚ (+ LLM)  â”‚              â”‚
â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  Groq/OpenAI   â”‚  â”‚  Orchestrator       â”‚            â”‚
â”‚  â”‚  LLM Client    â”‚  â”‚  (LLM reasoning     â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   at every step)     â”‚            â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  DuckDB (analytics)  â”‚  SQLite (state)   â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Tech Stack

- **Python 3.11+** / **FastAPI** â€” backend API + agent orchestration
- **Streamlit** â€” frontend UI (4 pages)
- **Groq** (or OpenAI) â€” LLM reasoning (llama-3.3-70b-versatile, free tier)
- **DuckDB** â€” in-memory analytics engine (loaded from CSV seed data)
- **SQLite** â€” persistence for feedback, evals, memory, traces, cases
- **Plotly** â€” interactive charts
- **Pydantic** â€” data models and validation

---

## Project Structure

```
opsiq/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                 # FastAPI application
â”‚   â”œâ”€â”€ config.py               # Settings from .env (LLM keys, server)
â”‚   â”œâ”€â”€ models/schemas.py       # Pydantic models
â”‚   â”œâ”€â”€ api/                    # REST endpoints
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ orchestrator.py     # LLM-powered autonomous pipeline
â”‚   â”‚   â”œâ”€â”€ monitor_agent.py    # Signal ingestion
â”‚   â”‚   â”œâ”€â”€ triage_agent.py     # Anomaly detection â†’ scoring â†’ cases
â”‚   â”‚   â”œâ”€â”€ analyst_agent.py    # Business Q&A
â”‚   â”‚   â”œâ”€â”€ evaluator_agent.py  # LLM-powered quality scoring
â”‚   â”‚   â””â”€â”€ memory_agent.py     # LLM-powered feedback â†’ memory updates
â”‚   â”œâ”€â”€ tools/                  # Anomaly detectors, scoring, SQL, charts
â”‚   â”œâ”€â”€ adapters/               # Signal, metric, action, sentiment, LLM
â”‚   â”œâ”€â”€ services/               # DuckDB data loader
â”‚   â””â”€â”€ storage/                # SQLite persistence layer
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ streamlit_app.py        # 4-page Streamlit UI
â”œâ”€â”€ data/                       # Seed CSV data with planted anomalies
â”œâ”€â”€ tests/                      # 133 tests (schemas, adapters, agents, API)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## Setup

### Prerequisites
- Python 3.11+

### Install

```bash
cd opsiq
pip install -r requirements.txt
```

### Configure

```bash
cp .env.example .env
```

Edit `.env`:

```env
# LLM reasoning (free â€” recommended)
GROQ_API_KEY=your_groq_key    # https://console.groq.com â†’ API Keys

# Or use OpenAI instead
OPENAI_API_KEY=your_key        # optional fallback
```

> **No LLM key?** The system still works â€” agents fall back to deterministic rule-based logic. With an LLM key, the agents reason about signals, synthesize findings, and learn from feedback intelligently.

### Seed Data (if CSVs are missing)

```bash
python data/seed_data.py
```

---

## Run

### 1. Start Backend

```bash
cd opsiq
python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

### 2. Start Frontend (separate terminal)

```bash
cd opsiq
python -m streamlit run frontend/streamlit_app.py --server.port 8501
```

### 3. Run Tests

```bash
python -m pytest tests/ -v
```

### 4. Open Browser

- **Frontend:** http://localhost:8501
- **API Docs:** http://localhost:8000/docs

---

## Deploy (Render + Streamlit Community Cloud)

This is the simplest production setup for OpsIQ:

- **Backend (FastAPI)** on Render
- **Frontend (Streamlit)** on Streamlit Community Cloud

### 1) Deploy backend to Render

Create a new **Web Service** from your GitHub repo.

- **Root Directory:** `opsiq`
- **Build Command:** `pip install -r requirements.txt`
- **Start Command:** `uvicorn app.main:app --host 0.0.0.0 --port $PORT`

Set env vars in Render:

- `GROQ_API_KEY` (recommended)
- `OPENAI_API_KEY` (optional)

After deploy, copy your backend URL, e.g.:

`https://opsiq-backend.onrender.com`

### 2) Deploy frontend to Streamlit Community Cloud

Create a new Streamlit app pointing to this repo:

- **Main file path:** `opsiq/frontend/streamlit_app.py`

In Streamlit app settings, add secrets/environment variables:

```toml
BACKEND_URL="https://opsiq-backend.onrender.com"
```

This is read by the frontend at runtime via `BACKEND_URL`.

### 3) Verify deployment

1. Open backend docs: `https://<your-backend>/docs`
2. Open Streamlit app
3. Run "Mission Control" investigation
4. Confirm cases, traces, and analyst queries all work

### Notes

- Render free tier may cold-start; first API call can take a few seconds.
- SQLite storage is ephemeral on many free hosts. For durable production state, move to a managed DB.

---

## Walkthrough

1. **Mission Control** â€” Click "Run Autonomous Investigation"
   - LLM analyzes signals and decides investigation strategy
   - ~6 anomaly cases detected, ranked by impact
   - Remediation actions created (case, alert, approval task)
   - Full **LLM reasoning trace** visible at every step

2. **Triage Cases** â€” Review cases, mark one as **False Positive**
   - See evidence, recommended action, sentiment risk score
   - Feedback triggers the self-improvement loop

3. **QA Lab** â€” See the learning in action
   - Memory updated: LLM decides which thresholds to adjust and by how much
   - Evaluation: LLM analyzes calibration and suggests improvements
   - Full reasoning log for observability

4. **Triage Cases** â€” Click "Rerun with Memory"
   - False-positive case now shows **lower confidence** (was high â†’ medium)
   - Impact reduced by 15% penalty
   - System learned from one interaction

5. **Analyst** â€” Ask "Why is revenue down this month?"
   - Get answer with chart, SQL, confidence, follow-ups

---

## API Endpoints

| Method | Path | Description |
|--------|------|-------------|
| GET | `/health` | Health check |
| POST | `/monitor/run` | Run autonomous investigation |
| GET | `/monitor/signals` | Fetch all signals |
| GET | `/triage/cases` | List all cases (with sentiment scores) |
| POST | `/triage/rerun` | Rerun with updated memory |
| POST | `/analyst/query` | Ask a business question |
| POST | `/feedback` | Submit feedback |
| GET | `/feedback/improvement` | Self-improvement summary |
| GET | `/eval/latest` | Latest evaluation |
| GET | `/llm/status` | LLM provider status |
| GET | `/llm/reasoning` | Full LLM reasoning log |
| POST | `/sentiment/analyze` | Analyze text sentiment |
| GET | `/sentiment/log` | Sentiment analysis audit trail |
| GET | `/memory` | Current memory state |
| GET | `/traces/latest` | Latest run trace |
| POST | `/demo/reset` | Reset all state |

---

## Self-Improvement Loop

```
User Feedback â†’ Memory Agent (LLM) â†’ Memory Store â†’ Triage Agent (rerun)
                      â†“
                Evaluator Agent (LLM) â†’ Eval Store â†’ QA Lab UI
```

**What changes on rerun after feedback:**
- LLM reasons about feedback â†’ decides which thresholds to adjust and by how much
- `false_positive_penalty` increases â†’ confidence downgraded for that anomaly type
- Type-specific thresholds adjust (e.g. duplicate window narrows, underbilling threshold rises)
- Scoring tool applies penalty â†’ lower impact scores
- Evaluator recalculates correctness and generates calibration advice

---

## Graceful Degradation

| Component | With API Key | Without API Key |
|-----------|-------------|-----------------|
| **LLM (Groq/OpenAI)** | Real AI reasoning at every decision point | Deterministic fallback (rule-based) |

The system is **fully functional without any API keys** â€” every feature works with deterministic logic. With a Groq key (free), the agents become truly intelligent.

---

## License

MIT
